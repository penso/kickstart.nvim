return {
  -- 'tzachar/cmp-ai',
  dir = "~/tmp/cmp-ai",
  dependencies = {
    'nvim-lua/plenary.nvim'
  },
  config = function()
    local cmp_ai = require('cmp_ai.config')

    cmp_ai:setup({
      max_lines = 100,
      provider = 'LmStudio',
      provider_options = {
        model = 'qwen2.5-coder-3b-instruct', -- qwen2.5-coder', -- codellama:7b-code',
        base_url = 'http://m4max.local:11434/v1/chat/completions',
        -- base_url = 'http://m4max.local:1234/v1/chat/completions',
        -- model = 'qwen2.5-coder-3b-instruct',
        -- auto_unload = false, -- Set to true to automatically unload the model when exiting nvim.
        raw_response_cb = function(response)
          vim.notify(vim.inspect(response))
        end,
        -- prompt = function(lines_before, lines_after)
        --   return "<think>" .. lines_before .. "</think>" .. lines_after .. "<|fim_middle|>"
        -- end,
        prompt = function(lines_before, lines_after)
          -- You may include filetype and/or other project-wise context in this string as well.
          -- Consult model documentation in case there are special tokens for this.
          return "<|fim_prefix|>" .. lines_before .. "<|fim_suffix|>" .. lines_after .. "\n```<end_code_middle>"
        end,
      },
      notify = true,
      notify_callback = function(msg)
        vim.notify(msg)
      end,
      run_on_every_keystroke = true,
      ignored_file_types = {
        -- default is not to ignore
        -- uncomment to ignore in lua:
        -- lua = true
      },
    })
  end
}
